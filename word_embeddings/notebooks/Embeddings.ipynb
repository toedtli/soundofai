{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4gZxaW8xaJ1j",
    "outputId": "29a80778-ec6a-4af9-ec3e-62300244f2f6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import gensim\r\n",
    "from transformers import AutoTokenizer\r\n",
    "from sentence_transformers import SentenceTransformer\r\n",
    "\r\n",
    "\r\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2DRoZw17aM5p"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade gensim\r\n",
    "#!pip install --upgrade pandas\r\n",
    "#!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bx989q0aY4t"
   },
   "outputs": [],
   "source": [
    "documents = [\r\n",
    "    \"Human machine interface for lab abc computer applications\",\r\n",
    "    \"A survey of user opinion of computer system response time\",\r\n",
    "    \"The EPS user interface management system\",\r\n",
    "    \"System and human system engineering testing of EPS\",\r\n",
    "    \"Relation of user perceived response time to error measurement\",\r\n",
    "    \"The generation of random binary unordered trees\",\r\n",
    "    \"The intersection graph of paths in trees\",\r\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\r\n",
    "    \"Graph minors A survey\",\r\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbUNxyL_a8k1",
    "outputId": "5e637dd8-22a5-455f-b835-6a5b136a72b7"
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords,preprocess_documents\r\n",
    "from pprint import pprint\r\n",
    "pprint([remove_stopwords(doc) for doc in documents])\r\n",
    "documents2 = preprocess_documents(documents)\r\n",
    "documents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CFzJyz31cfZl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jukgIfdseEsW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8HKN5Gdcf0q"
   },
   "source": [
    "# Word2Vec\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMZBxLZ4ckVS",
    "outputId": "f2bd0dc8-ad12-4e65-8553-67f4fb7322ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecsize=128\r\n",
    "documents\r\n",
    "from gensim.test.utils import common_texts\r\n",
    "\r\n",
    "\r\n",
    "documents\r\n",
    "common_texts\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yNFYzF39cewi"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "model = Word2Vec(sentences=common_texts, size=vecsize, window=5, min_count=1, workers=4)\r\n",
    "model = Word2Vec(sentences=documents, size=vecsize, window=5, min_count=1, workers=4)\r\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcEWHamvhR0R"
   },
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nEbFo-12akrS"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\r\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
    "#TaggedDocument?\r\n",
    "\r\n",
    "#This I don't understand:\r\n",
    "#\"Tags may be one or more unicode string tokens, but typical practice (which will also be the most memory-efficient) is for the tags list to include a unique integer id as the only tag.\"\r\n",
    "#What kind of meaning do these tags then contain?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9SiQ3ZejdQ3_"
   },
   "outputs": [],
   "source": [
    "documents3 = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\r\n",
    "model = Doc2Vec(documents3, vector_size=128, window=2, min_count=1, workers=4)\r\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysqHDtmTb8L8",
    "outputId": "c23e3f76-4b97-40ed-cf15-253b86344a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 382 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\r\n",
    "vector = model.infer_vector([\"system\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74EwBNWWgQUe",
    "outputId": "3fe51ec6-7962-4a0b-df72-02b90a4e8183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.infer_vector([\"system\", \"response\"])\r\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srLZBqXRhtb3"
   },
   "source": [
    "## Bert\r\n",
    "Choose a model from here: [https://huggingface.co/models](https://huggingface.co/models).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "So_LcYpchxev"
   },
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "!pip install -U Huggingface\r\n",
    "!pip install -U sentence-transformers\r\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cwTaS276gUMt"
   },
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
    "sbert_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "4lfBsFjQif4z",
    "outputId": "1904b56c-59f3-4ecc-cb41-1096d90381df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[This framework generates embeddings for each ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Sentences are passed as a list of string.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The quick brown fox jumps over the lazy dog.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Well, with another iterator it works.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   documents_cleaned\n",
       "0  [This framework generates embeddings for each ...\n",
       "1        [Sentences are passed as a list of string.]\n",
       "2  [The quick brown fox jumps over the lazy dog.,...\n",
       "3            [Well, with another iterator it works.]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['This framework generates embeddings for each input sentence'],\r\n",
    "    ['Sentences are passed as a list of string.'], \r\n",
    "    ['The quick brown fox jumps over the lazy dog.','Here I am wondering how multi-sentence documents are used.','This seems not the right way. Probably a list of lists of words is necessary?' ],\r\n",
    "    ['Well, with another iterator it works.' ]]\r\n",
    "documents_df = pd.DataFrame({'documents_cleaned': sentences})\r\n",
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uGwPVFNujLbV",
    "outputId": "fc1d2baa-89f6-4b75-ccc4-f98baf11bd08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "(1, 768)\n",
      "(3, 768)\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = [sbert_model.encode(doc) for doc in documents_df.documents_cleaned]\r\n",
    "\r\n",
    "print(\"Sentence embeddings:\")\r\n",
    "print(sentence_embeddings[0].shape)\r\n",
    "print(sentence_embeddings[-2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "piU1bbh0lXPx",
    "outputId": "ac19a38f-dffa-4b46-a73f-049fce098abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7592, 1010, 1045, 1005, 1049, 1037, 2309, 6251, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] hello, i'm a single sentence! [SEP]\""
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "\r\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\r\n",
    "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\r\n",
    "print(encoded_input)\r\n",
    "decoded_input = tokenizer.decode(encoded_input[\"input_ids\"])\r\n",
    "decoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BrPUnGMYjtuN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "qc04Z82ojZqo",
    "outputId": "c09e387d-2493-45df-a2a0-bff060f0c73a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[This framework generates embeddings for each ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Sentences are passed as a list of string.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The quick brown fox jumps over the lazy dog.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Well, with another iterator it works.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   documents_cleaned\n",
       "0  [This framework generates embeddings for each ...\n",
       "1        [Sentences are passed as a list of string.]\n",
       "2  [The quick brown fox jumps over the lazy dog.,...\n",
       "3            [Well, with another iterator it works.]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_CQqqaXhwT0",
    "outputId": "5a33d11a-ed08-48dd-edda-5a62627ee9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "[[-0.10409425  0.5274767   1.1797731  ... -0.43389115 -0.6945233\n",
      "   0.5386927 ]\n",
      " [-0.13118456 -0.17390285  1.1052188  ...  0.02624461 -0.00269846\n",
      "   0.916111  ]\n",
      " [-0.7489929   0.71891737 -1.039457   ...  0.15582633  1.0202514\n",
      "   0.09790451]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\r\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
    "sentences = ['This framework generates embeddings for each input sentence',\r\n",
    "    'Sentences are passed as a list of string.', \r\n",
    "    'The quick brown fox jumps over the lazy dog.']\r\n",
    "sentence_embeddings = model.encode(sentences)\r\n",
    "\r\n",
    "print(\"Sentence embeddings:\")\r\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M00wzG_YsTPa"
   },
   "source": [
    "# Using Mirco's Benchmarking Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQf7-L1GrC0J",
    "outputId": "939aced5-d2fc-456b-d922-1bca1dd9bf5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "/home\n"
     ]
    }
   ],
   "source": [
    "!pwd -P\r\n",
    "%cd /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tKuJYVYUsWXk",
    "outputId": "ce9c9e5c-8ab4-4ed5-c6a2-e2a6ef1cd4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\r\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkhzMAAQsv71",
    "outputId": "71ccbf66-1bce-4cc0-ecb0-bc25c991704e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/MyDrive/Colab Notebooks/soundofai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Jn681khykXV",
    "outputId": "11904cc8-244e-47ad-8a33-e50fdcccb587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'soundofai' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --branch toedtli_mytestbranch https://toedtli:m2y7s4upers73ecret%21password@github.com/toedtli/soundofai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3WnROXPtJR9",
    "outputId": "a5f61186-06ee-49e7-a9a4-d9e8956853ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: toedtli\n",
      "Password: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "branch name: toedtli_mytestbranch\n"
     ]
    }
   ],
   "source": [
    "!rm -rf soundofai\r\n",
    "import os\r\n",
    "from getpass import getpass\r\n",
    "import urllib\r\n",
    "\r\n",
    "user = input('User name: ')\r\n",
    "password = getpass('Password: ')\r\n",
    "password = urllib.parse.quote(password) # your password is converted into url format\r\n",
    "#repo_name = input('Repo name: ')\r\n",
    "repo_name = 'soundofai'\r\n",
    "branch = input('branch name: ')\r\n",
    "cmd_string = f'git clone --branch {branch} https://{user}:{password}@github.com/{user}/{repo_name}.git'\r\n",
    "\r\n",
    "os.system(cmd_string)\r\n",
    "cmd_string, password = \"\", \"\" # removing the password from the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcIvF5MlwK8Z",
    "outputId": "f5bbf780-ca20-45c9-8d9c-1dc1c83bb8db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t\t\ttext_embedders_benchmark_preview.py\n",
      "text_embedders_benchmark_preview.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls soundofai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsxwZc9rwY1y",
    "outputId": "55649214-b4e4-45b7-bd44-07aa38e7e128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/soundofai\n"
     ]
    }
   ],
   "source": [
    "%cd /home/soundofai\r\n",
    "#!pip install tensorflow_text\r\n",
    "from text_embedders_benchmark_preview import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TWDympjX2XxG"
   },
   "outputs": [],
   "source": [
    "class GensimPredictionModelWithPreprocessor(PredictionModel):\r\n",
    "  family='Gensim'\r\n",
    "  def build(self):\r\n",
    "    #text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\r\n",
    "    #preprocessor = hub.KerasLayer(self.preprocessor_url)\r\n",
    "    self.preprocessor = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\r\n",
    "    encoder_inputs = self.preprocessor\r\n",
    "    self.model = SentenceTransformer('bert-base-nli-mean-tokens')\r\n",
    "\r\n",
    "  def predict(self, sentences):\r\n",
    "    output_tensor = self.model.encode(sentences)\r\n",
    "  \r\n",
    "    return output_tensor\r\n",
    "\r\n",
    "  def additional_infos(self):\r\n",
    "        return {\r\n",
    "        \"general\":\"This is not a tf-hub model, but a gensim model. maybe need a type key here\",\r\n",
    "        \"tf_hub_url\":None,\r\n",
    "        \"family\":self.family,\r\n",
    "        \"word_level_output_available\":True\r\n",
    "    }\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5RdOLiWiB3Tb"
   },
   "outputs": [],
   "source": [
    "def benchmark_prediction_model(model_name, sentences, results=None):\r\n",
    "  model=eval(f\"{model_name}()\")\r\n",
    "\r\n",
    "  if results is None:\r\n",
    "    results={}\r\n",
    "  \r\n",
    "  results[\"model_name\"]=model_name\r\n",
    "  \r\n",
    "  print(f\"{model_name} - building...\")\r\n",
    "  now=time.time()\r\n",
    "  model.build()\r\n",
    "  results[\"build_seconds\"]=time.time()-now\r\n",
    "  \r\n",
    "  print(f\"{model_name} - first prediction...\")\r\n",
    "  now=time.time()\r\n",
    "  prediction = model.predict(sentences)\r\n",
    "  results[\"first_prediction_seconds\"]=time.time()-now\r\n",
    "  \r\n",
    "  print(f\"{model_name} - second prediction...\")\r\n",
    "  now=time.time()\r\n",
    "  prediction = model.predict(sentences)\r\n",
    "  results[\"second_prediction_seconds\"]=time.time()-now\r\n",
    "\r\n",
    "  results[\"embedding_size\"]=prediction.shape[1]\r\n",
    "  results[\"additional_infos\"]=json.dumps(model.additional_infos())\r\n",
    "\r\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJFWV9JhDcbM",
    "outputId": "67bce236-2cc4-43e3-fc22-882f9f41c477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46516186, -0.579506  ,  0.23042098, ...,  0.6943612 ,\n",
       "        -0.13120279, -0.73131865],\n",
       "       [-0.7033002 , -0.5974654 ,  0.5133683 , ..., -0.05653809,\n",
       "        -0.18607607,  0.35441774],\n",
       "       [-0.04879691, -0.01012288,  0.47366896, ...,  0.72819793,\n",
       "        -0.07263067,  0.06405059],\n",
       "       ...,\n",
       "       [-0.31490332,  0.7199607 , -0.13828947, ...,  0.05519799,\n",
       "         0.33923128, -0.1394053 ],\n",
       "       [ 0.01398076,  0.33343962,  0.91195005, ...,  1.1070508 ,\n",
       "        -0.23172055,  0.07168981],\n",
       "       [ 0.10315251, -0.5857194 ,  1.0750997 , ..., -0.32626587,\n",
       "        -0.44016764, -0.8372998 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GensimPredictionModelWithPreprocessor()\r\n",
    "model.build()\r\n",
    "model.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYGZyFM0CWxI",
    "outputId": "ca17a01c-d35d-4c9e-b6a9-c6e0cd509483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GensimPredictionModelWithPreprocessor - building...\n",
      "GensimPredictionModelWithPreprocessor - first prediction...\n",
      "GensimPredictionModelWithPreprocessor - second prediction...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'additional_infos': '{\"general\": \"This is not a tf-hub model, but a gensim model. maybe need a type key here\", \"tf_hub_url\": null, \"family\": \"Gensim\", \"word_level_output_available\": true}',\n",
       " 'build_seconds': 4.127309560775757,\n",
       " 'embedding_size': 768,\n",
       " 'first_prediction_seconds': 1.590348720550537,\n",
       " 'model_name': 'GensimPredictionModelWithPreprocessor',\n",
       " 'second_prediction_seconds': 1.6815154552459717}"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_prediction_model('GensimPredictionModelWithPreprocessor', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxpHphIVCTQW",
    "outputId": "3a758073-785a-43d5-bfd0-415ad2eabe3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': False}"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_embedders_benchmark_preview import *\r\n",
    "tools=BenchmarkingTools()\r\n",
    "p = 'GensimPredictionModelWithPreprocessor'\r\n",
    "safe_benchmark_prediction_model(GensimPredictionModelWithPreprocessor,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzVsiIsc94Q6",
    "outputId": "4cc7b4d5-909e-462e-c722-a30195c4e762"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give me a bright guitar',\n",
       " \"I'd like a sharp cello\",\n",
       " 'give me a dry acoustic guitar',\n",
       " 'give me a metallic harp',\n",
       " 'give me a dirty organ',\n",
       " 'give me a hollow piano',\n",
       " 'give me a sharp trumpet',\n",
       " 'give me a cold triangle',\n",
       " 'give me dark drums',\n",
       " 'give me a soft french horn',\n",
       " 'give me a dull clarinet',\n",
       " 'give me a smooth operator',\n",
       " 'Give me a simple square bass',\n",
       " 'Give me an orchestral string',\n",
       " 'Give me an analog pad',\n",
       " 'Give me a simple sine bass',\n",
       " 'Give me a chord preset',\n",
       " 'Get me a 909 closed hi-hat',\n",
       " 'Get me an 808 open hi-hat',\n",
       " 'Give me a round bass',\n",
       " 'Give me a sharp synth',\n",
       " 'Give me a warm pad',\n",
       " 'Give me a wide stereo pad',\n",
       " 'Give me a mono, warm, round synth bass',\n",
       " 'Make me a soft flute that sounds like a chirping bird ',\n",
       " 'Give me a dark brassy sound',\n",
       " 'Can you give me a wailing guitar?',\n",
       " 'Get me a scratchy violin',\n",
       " 'Give me a Star Wars laser beam sound',\n",
       " 'Can you combine a low piano sound with a roaring lion?',\n",
       " 'Get me something like a compact bleep',\n",
       " 'Give me a funky guitar']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "PWDEbC1Y-8UB",
    "outputId": "70cce13f-b6fc-4557-fed7-f3e0158a3833"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-0445ab8a71a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark_prediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GensimPredictionModelWithPreprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-d5ba3ceac388>\u001b[0m in \u001b[0;36mbenchmark_prediction_model\u001b[0;34m(model_name, sentences, results)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark_prediction_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class GensimPredictionModelWithPreprocessor with abstract methods predict"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "MCXYgAMaB8WE",
    "outputId": "f85dfb7d-3ed6-451f-9206-8b081ebf721d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-f9a37e060650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGensimPredictionModelWithPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class GensimPredictionModelWithPreprocessor with abstract methods predict"
     ]
    }
   ],
   "source": [
    "GensimPredictionModelWithPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d14dae7737b34168a1840ff4e23b98e7",
      "4d3676acbc6f413dbb8ba37c1646ec65",
      "25a6c8bc3c284b67972e7786ea347427",
      "a0c4f9498f2f482b94683c953a1f8771",
      "85af185adb28417da414d275427a0c8e",
      "16815a9aaf714f7f9fd96c466a672586",
      "549abc4fd1bc432ab979629a3018e0e0",
      "087e61c00b284fc2a665c9f5aa6052d1"
     ]
    },
    "id": "yt-lJgEE1Owl",
    "outputId": "1327c89e-775d-4ed5-8fb0-58815102d387"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14dae7737b34168a1840ff4e23b98e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talkheads_ggelu_bert_en_large - building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
      "INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3, Total size: 1.96MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1'.\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 504, in safe_benchmark_prediction_model\n",
      "    benchmark_prediction_model(model_name, sentences, results)\n",
      "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 480, in benchmark_prediction_model\n",
      "    model.build()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-9a4156ac7630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark_and_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m#r=benchmark_prediction_model(p, sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/soundofai/text_embedders_benchmark_preview.py\u001b[0m in \u001b[0;36mbenchmark_and_cleanup\u001b[0;34m(self, model_name, sentences)\u001b[0m\n\u001b[1;32m    526\u001b[0m         target=safe_benchmark_prediction_model, args=(model_name, sentences, return_dict))\n\u001b[1;32m    527\u001b[0m     \u001b[0mprocess_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mprocess_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 76, in build\n",
      "    encoder = hub.KerasLayer(self.tf_hub_url, trainable=False)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 152, in __init__\n",
      "    self._func = load_module(handle, tags, self._load_options)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\", line 427, in load_module\n",
      "    return module_v2.load(handle, tags=tags, options=set_load_options)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 92, in load\n",
      "    module_path = resolve(handle)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\", line 47, in resolve\n",
      "    return registry.resolver(handle)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/registry.py\", line 51, in __call__\n",
      "    return impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 68, in __call__\n",
      "    self._lock_file_timeout_sec())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\", line 409, in atomic_download\n",
      "    download_fn(handle, tmp_dir)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/compressed_module_resolver.py\", line 65, in download\n",
      "    response, tmp_dir)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\", line 190, in download_and_uncompress\n",
      "    fileobj, dst_path, log_function=self._log_progress)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/file_utils.py\", line 52, in extract_tarfile_to_destination\n",
      "    extract_file(tgz, tarinfo, abs_target_path, log_function=log_function)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_hub/file_utils.py\", line 35, in extract_file\n",
      "    buf = src.read(buffer_size)\n",
      "  File \"/usr/lib/python3.6/tarfile.py\", line 708, in readinto\n",
      "    buf = self.read(len(b))\n",
      "  File \"/usr/lib/python3.6/tarfile.py\", line 697, in read\n",
      "    b = self.fileobj.read(length)\n",
      "  File \"/usr/lib/python3.6/tarfile.py\", line 539, in read\n",
      "    buf = self._read(size)\n",
      "  File \"/usr/lib/python3.6/tarfile.py\", line 556, in _read\n",
      "    buf = self.cmp.decompress(buf)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/soundofai/text_embedders_benchmark_preview.py\", line 507, in safe_benchmark_prediction_model\n",
      "    results[\"success\"]=False\n",
      "  File \"<string>\", line 2, in __setitem__\n",
      "  File \"/usr/lib/python3.6/multiprocessing/managers.py\", line 757, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "prediction_models\r\n",
    "tools=BenchmarkingTools()\r\n",
    "results = []\r\n",
    "for p in tqdm(prediction_models):\r\n",
    "  r=tools.benchmark_and_cleanup(p, sentences)\r\n",
    "  #r=benchmark_prediction_model(p, sentences)\r\n",
    "  results.append(r)\r\n",
    "\r\n",
    "df=pd.DataFrame(results)\r\n",
    "df.to_csv(\"results.csv\", index=False)\r\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3wxorzL1vBE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "087e61c00b284fc2a665c9f5aa6052d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16815a9aaf714f7f9fd96c466a672586": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25a6c8bc3c284b67972e7786ea347427": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16815a9aaf714f7f9fd96c466a672586",
      "max": 56,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85af185adb28417da414d275427a0c8e",
      "value": 0
     }
    },
    "4d3676acbc6f413dbb8ba37c1646ec65": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "549abc4fd1bc432ab979629a3018e0e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85af185adb28417da414d275427a0c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a0c4f9498f2f482b94683c953a1f8771": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_087e61c00b284fc2a665c9f5aa6052d1",
      "placeholder": "â",
      "style": "IPY_MODEL_549abc4fd1bc432ab979629a3018e0e0",
      "value": " 0/56 [56:52&lt;?, ?it/s]"
     }
    },
    "d14dae7737b34168a1840ff4e23b98e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25a6c8bc3c284b67972e7786ea347427",
       "IPY_MODEL_a0c4f9498f2f482b94683c953a1f8771"
      ],
      "layout": "IPY_MODEL_4d3676acbc6f413dbb8ba37c1646ec65"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
